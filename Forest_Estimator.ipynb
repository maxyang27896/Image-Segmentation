{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user name depending on computer\n",
    "username = os.getlogin()\n",
    "\n",
    "# path\n",
    "image_test_path = os.path.join (r'P:\\Projects\\SPR998.41 - ES&S\\Jetson TX2 Investigation\\Working\\Dataset\\test\\image')\n",
    "mask_test_path = os.path.join (r'P:\\Projects\\SPR998.41 - ES&S\\Jetson TX2 Investigation\\Working\\Dataset\\test\\mask')\n",
    "image_train_path = os.path.join (r'P:\\Projects\\SPR998.41 - ES&S\\Jetson TX2 Investigation\\Working\\Dataset\\train\\image')\n",
    "mask_train_path = os.path.join (r'P:\\Projects\\SPR998.41 - ES&S\\Jetson TX2 Investigation\\Working\\Dataset\\train\\mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images \n",
    "def load_image(file_path, size):\n",
    "    image_array = []\n",
    "    \n",
    "    for filename in os.listdir(file_path):\n",
    "               \n",
    "        # get mask path and read mask\n",
    "        image_path = os.path.join(file_path, filename)\n",
    "        image_raw = cv2.imread(image_path)/255\n",
    "          \n",
    "        # Append to list\n",
    "        image_array.append(tf.image.resize(image_raw, size))\n",
    "        \n",
    "    return np.stack(image_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load masks\n",
    "def load_mask(file_path, size):\n",
    "    mask_array = []\n",
    "    \n",
    "    for filename in os.listdir(file_path):\n",
    "        \n",
    "        # get mask path and read mask\n",
    "        image_path = os.path.join(file_path, filename)\n",
    "        image_raw = cv2.imread(image_path)\n",
    "        image_raw = np.array(tf.image.resize(image_raw, size, method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)).astype('int32')\n",
    "        \n",
    "        # filter mask into single channel categories\n",
    "        image_out = np.zeros([image_raw.shape[0],image_raw.shape[1],1])\n",
    "        \n",
    "        image_out[np.where(image_raw[:,:,1]==255)] = 1   # Grass Pixel [0,255,0]\n",
    "        image_out[np.where(image_raw[:,:,0]==51)] = 2    # Vegetation/tree [51,102,102]\n",
    "        image_out[np.where(image_raw[:,:,0]==170)] = 3   # Roads [170,170,170]\n",
    "        image_out[np.where(image_raw[:,:,0]==255)] = 4   # sky [255,120,0]     \n",
    "    \n",
    "        # Append to list\n",
    "        mask_array.append(image_out)\n",
    "        \n",
    "    return  np.stack(mask_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [128, 128]\n",
    "image_test = load_image(image_test_path, size)\n",
    "mask_test = load_mask(mask_test_path, size)\n",
    "image_train = load_image(image_train_path, size)\n",
    "mask_train = load_mask(mask_train_path, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full data set\n",
    "image_array = np.concatenate((image_train, image_test), axis=0)\n",
    "mask_array = np.concatenate((mask_train, mask_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "num_image = len(image_array)\n",
    "num_mask = len(mask_array)\n",
    "\n",
    "if num_image != num_mask:\n",
    "    raise ImportError('Image data and mask data do not match!')\n",
    "else:\n",
    "     train_size = num_image   \n",
    "    \n",
    "print(num_image)\n",
    "print(num_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "batch_size = 32\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation techniques\n",
    "# Flip the image randomly\n",
    "@tf.function\n",
    "def image_flip(input_image, input_mask):\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "        \n",
    "#     if tf.random.uniform(()) > 0.5:\n",
    "#         input_image = tf.image.rot90(input_image, k=1)\n",
    "#         input_mask = tf.image.rot90(input_mask, k=1)\n",
    "        \n",
    "#     if tf.random.uniform(()) > 0.5:\n",
    "#         input_image = tf.image.flip_up_down(input_image)\n",
    "#         input_mask = tf.image.flip_up_down(input_mask)\n",
    "    \n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "# Change the the colour of image randomly\n",
    "@tf.function\n",
    "def image_color(input_image, input_mask):\n",
    "\n",
    "    input_image = tf.image.random_hue(input_image, 0.08)\n",
    "    input_image = tf.image.random_saturation(input_image, 0.6, 1.6)\n",
    "    input_image = tf.image.random_brightness(input_image, 0.05)\n",
    "    input_image = tf.image.random_contrast(input_image, 0.7, 1.3)\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip image and make sure image and mask are in the correct dtype\n",
    "@tf.function\n",
    "def clip_images(input_image,input_mask):\n",
    "    return (tf.cast(tf.clip_by_value(input_image, 0, 1), tf.float32), tf.cast(input_mask,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transform(dataset, augmentations):\n",
    "        \n",
    "    # Add the augmentations to the dataset\n",
    "    for f in augmentations:\n",
    "        # Apply the augmentation, run 2 jobs in parallel.\n",
    "        dataset = dataset.map(f, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "   \n",
    "    dataset = dataset.map(clip_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model downsampling \n",
    "OUTPUT_CHANNELS = 5\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=size + [3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model downsampling \n",
    "OUTPUT_CHANNELS = 5\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=size + [3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define upsampling using pix2pix.upsample(#filters, filter_size)\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]\n",
    "\n",
    "def unet_model(output_channels):\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='valid', activation='softmax')  #64x64 -> 128x128\n",
    "    \n",
    "    last_last = tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0)), data_format=\"channels_last\")\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=size + [3])\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "    x = last_last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 [(None, 64, 64, 96), 1841984     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 8, 8, 512)    1476608     model_1[1][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 1088)   0           sequential[0][0]                 \n",
      "                                                                 model_1[1][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 16, 16, 256)  2507776     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 448)  0           sequential_1[0][0]               \n",
      "                                                                 model_1[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 32, 32, 128)  516608      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 272)  0           sequential_2[0][0]               \n",
      "                                                                 model_1[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 64, 64, 64)   156928      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 160)  0           sequential_3[0][0]               \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 129, 129, 5)  7205        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 128, 128, 5)  0           conv2d_transpose_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 6,507,109\n",
      "Trainable params: 4,663,205\n",
      "Non-trainable params: 1,843,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "WARNING:tensorflow:From C:\\Users\\MY2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './Forest_CKPT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002113B4B3D30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# the model_dir states where the graph and checkpoint files will be saved to\n",
    "estimator_model = tf.keras.estimator.model_to_estimator(keras_model = model, \\\n",
    "                                                        model_dir = './Forest_CKPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_3: (None, 128, 128, 3)}, (None, 128, 128, 1)), types: ({input_3: tf.float32}, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another input function for from slices\n",
    "def input_function(features,labels=None,shuffle=False, batch_size = 128):\n",
    "    \n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    \n",
    "    # Map the train dataset with tranformation for data augmentation\n",
    "    if shuffle:\n",
    "        dataset = dataset_transform(dataset, [image_flip,image_color])\n",
    "        \n",
    "    dataset = dataset.map(lambda features, labels: ({'input_3':features}, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "input_function(image_array, mask_array, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./Forest_CKPT\\\\keras\\\\keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ./Forest_CKPT\\keras\\keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 165 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./Forest_CKPT\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.9963233, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 46 into ./Forest_CKPT\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 92 into ./Forest_CKPT\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0761725\n",
      "INFO:tensorflow:loss = 0.21212316, step = 100 (1312.813 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 140 into ./Forest_CKPT\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 165 into ./Forest_CKPT\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1643649.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x21142897470>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING \n",
    "EPOCHS = 15\n",
    "STEPS = train_size // batch_size * EPOCHS\n",
    "\n",
    "estimator_model.train(input_fn = lambda: input_function(image_array, mask_array, True), steps = STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
