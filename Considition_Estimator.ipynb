{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get username\n",
    "username = os.getlogin()\n",
    "\n",
    "# Locate the training and mask directories\n",
    "image_dir = r\"C:\\Users\\\\\"+ username + r\"\\iCloudDrive\\Documents\\Computer Science\\Personal Projects\\Considition (Image Recongition)\\Training_dataset\\Image\\Images\"\n",
    "mask_dir = r\"C:\\Users\\\\\" + username + r\"\\iCloudDrive\\Documents\\Computer Science\\Personal Projects\\Considition (Image Recongition)\\Training_dataset\\Masks\\Mask\\all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(file_path, size):\n",
    "    \n",
    "    mask_array = []\n",
    "    \n",
    "    # loop throught each mask directory\n",
    "    for filename in os.listdir(file_path):\n",
    "               \n",
    "        # get mask path and read mask\n",
    "        img_path = os.path.join(file_path, filename)\n",
    "        image_cv = cv2.imread(img_path)\n",
    "        \n",
    "        # filter mask into single channel categories\n",
    "        np_image = np.array(tf.image.resize(image_cv, size, method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)).astype('int32')\n",
    "        np_image[:,:,0] = np.where(np_image[:,:,0] != 255, np_image[:,:,0], 510)\n",
    "        np_image = np.sum(np_image/255.0, axis = 2)\n",
    "        \n",
    "        mask_array.append(np_image)\n",
    "        \n",
    "    # expand the last dimension from to 1 for model requirements\n",
    "    mask_array = np.expand_dims(mask_array, 3) \n",
    "    \n",
    "    return mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image function\n",
    "def load_images(file_path,size):\n",
    "    \n",
    "    image_array = []\n",
    "    \n",
    "    for filename in os.listdir(file_path):\n",
    "        img_path = os.path.join(file_path, filename)\n",
    "        image_cv = cv2.imread(img_path)/255\n",
    "        np_image = cv2.resize(image_cv, size)\n",
    "        image_array.append(np_image)\n",
    "    \n",
    "    return np.asarray(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128,128)\n",
    "image_array = load_images(image_dir, size)\n",
    "mask_array = load_mask(mask_dir, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670\n",
      "1670\n"
     ]
    }
   ],
   "source": [
    "num_image = len(image_array)\n",
    "num_mask = len(mask_array)\n",
    "\n",
    "if num_image != num_mask:\n",
    "    raise ImportError('Image data and mask data do not match!')\n",
    "else:\n",
    "     data_size = num_image   \n",
    "    \n",
    "print(num_image)\n",
    "print(num_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "val_split = 0.2\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336\n",
      "334\n"
     ]
    }
   ],
   "source": [
    "# create train and test data size\n",
    "train_size = int((1-val_split) * data_size)\n",
    "test_size = int(data_size - train_size)\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the image randomly\n",
    "@tf.function\n",
    "def image_flip(input_image, input_mask):\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    \n",
    "    return input_image, input_mask\n",
    "\n",
    "# Change the the colour of image randomly\n",
    "@tf.function\n",
    "def image_color(input_image, input_mask):\n",
    "\n",
    "    input_image = tf.image.random_hue(input_image, 0.08)\n",
    "    input_image = tf.image.random_saturation(input_image, 0.6, 1.6)\n",
    "    input_image = tf.image.random_brightness(input_image, 0.05)\n",
    "    input_image = tf.image.random_contrast(input_image, 0.7, 1.3)\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip image and make sure image and mask are in the correct dtype\n",
    "@tf.function\n",
    "def clip_images(input_image,input_mask):\n",
    "    return (tf.cast(tf.clip_by_value(input_image, 0, 1), tf.float32), tf.cast(input_mask,tf.int16))\n",
    "\n",
    "# Function to combine all the data augmentation and processing functions\n",
    "def dataset_transform(dataset, augmentations):\n",
    "        \n",
    "    # Add the augmentations to the dataset\n",
    "    for f in augmentations:\n",
    "        # Apply the augmentation, run 2 jobs in parallel.\n",
    "        dataset = dataset.map(f, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "   \n",
    "    dataset = dataset.map(clip_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model downsampling \n",
    "OUTPUT_CHANNELS = 4\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define upsampling using pix2pix.upsample(#filters, filter_size)\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]\n",
    "\n",
    "def unet_model(output_channels):\n",
    "\n",
    "    # This is the last 2 layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='valid', activation='softmax') #64x64 -> 128x128\n",
    "    \n",
    "    last_last = tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0)), data_format=\"channels_last\")\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "    x = last_last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   [(None, 64, 64, 96), 1841984     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 8, 8, 512)    1476608     model[1][4]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 1088)   0           sequential[0][0]                 \n",
      "                                                                 model[1][3]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 16, 16, 256)  2507776     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 448)  0           sequential_1[0][0]               \n",
      "                                                                 model[1][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 32, 32, 128)  516608      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 272)  0           sequential_2[0][0]               \n",
      "                                                                 model[1][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 64, 64, 64)   156928      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 160)  0           sequential_3[0][0]               \n",
      "                                                                 model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 129, 129, 4)  5764        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 128, 128, 4)  0           conv2d_transpose_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 6,505,668\n",
      "Trainable params: 4,661,764\n",
      "Non-trainable params: 1,843,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cropping2d_4/Identity']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.op.name for node in model.outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "WARNING:tensorflow:From C:\\Users\\MY2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './Considition_CKPT_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AD94C9F0B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# the model_dir states where the graph and checkpoint files will be saved to\n",
    "estimator_model = tf.keras.estimator.model_to_estimator(keras_model = model, \\\n",
    "                                                        model_dir = './Considition_CKPT_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_2: (None, 128, 128, 3)}, (None, 128, 128, 1)), types: ({input_2: tf.float32}, tf.int16)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another input function for from slices\n",
    "def input_function(features,labels=None,shuffle=False, batch_size = 128):\n",
    "    \n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    \n",
    "    # Map the train dataset with tranformation for data augmentation\n",
    "    if shuffle:\n",
    "        dataset = dataset_transform(dataset, [image_flip,image_color])\n",
    "        \n",
    "    dataset = dataset.map(lambda features, labels: ({'input_2':features}, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "input_function(image_array, mask_array, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MY2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./Considition_CKPT_1\\\\keras\\\\keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ./Considition_CKPT_1\\keras\\keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 165 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./Considition_CKPT_1\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4932559, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 41 into ./Considition_CKPT_1\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5759331.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x1adce667f60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING \n",
    "EPOCHS = 1\n",
    "STEPS = train_size // batch_size * EPOCHS\n",
    "\n",
    "estimator_model.train(input_fn = lambda: input_function(image_array, mask_array, True), steps = STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
